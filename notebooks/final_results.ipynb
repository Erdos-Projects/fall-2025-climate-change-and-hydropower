{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c959d3-5e98-48f0-9449-75e6ad45b92f",
   "metadata": {},
   "source": [
    "# Final results from our neural network\n",
    "Code is taken from <strong><code>Jackson/Model-Test.py</code></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e810a7f9-d1db-49c8-9237-9e452e26e15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022 Test Results | RMSE: 45124.52, MAE: 10181.40, R²: 0.694\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# =============================\n",
    "# 0. Get scaler from 2001-2022\n",
    "# =============================\n",
    "def get_scaler():\n",
    "    df = pd.read_csv(\"../data/train_val_2001_2021.csv\")\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[numeric_cols])\n",
    "    return scaler\n",
    "\n",
    "# =============================\n",
    "# 1. Load test data (2022)\n",
    "# =============================\n",
    "test_df = pd.read_csv(\"../data/final_test_2022.csv\")  # Your 2022 data\n",
    "TARGET_COL = \"RectifHyd_MWh\"\n",
    "\n",
    "# Ensure numeric target\n",
    "test_df[TARGET_COL] = pd.to_numeric(test_df[TARGET_COL], errors='coerce')\n",
    "test_df = test_df.dropna(subset=[TARGET_COL])\n",
    "\n",
    "# Ensure numeric features\n",
    "numeric_cols = [c for c in test_df.columns if c not in [\"Division_ID\", \"Primary Purpose\", \"nerc_region\", \"mode\", TARGET_COL, 'year']]\n",
    "for col in numeric_cols:\n",
    "    test_df[col] = pd.to_numeric(test_df[col], errors='coerce')\n",
    "\n",
    "# =============================\n",
    "# 2. Load preprocessing objects\n",
    "# =============================\n",
    "with open(\"../artifacts/label_encoders.pkl\", \"rb\") as f:\n",
    "    label_encoders = pickle.load(f)\n",
    "with open(\"../artifacts/scaler.pkl\", \"rb\") as f:\n",
    "    scaler = get_scaler()\n",
    "\n",
    "categorical_cols = [\"Division_ID\", \"Primary Purpose\", \"nerc_region\", \"mode\"]\n",
    "\n",
    "# Fill missing numeric values with median from test set\n",
    "for col in numeric_cols:\n",
    "    test_df[col] = test_df[col].fillna(test_df[col].median())\n",
    "\n",
    "# Encode categorical features using training label encoders\n",
    "for col in categorical_cols:\n",
    "    le = label_encoders[col]\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "\n",
    "# Scale numeric features\n",
    "test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n",
    "\n",
    "# Convert to tensors\n",
    "X_numeric_test = torch.tensor(test_df[numeric_cols].values, dtype=torch.float32)\n",
    "X_categorical_test = torch.tensor(test_df[categorical_cols].values, dtype=torch.long)\n",
    "y_test = torch.tensor(test_df[TARGET_COL].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# =============================\n",
    "# 3. Dataset & DataLoader\n",
    "# =============================\n",
    "class DamEnergyDataset(Dataset):\n",
    "    def __init__(self, X_num, X_cat, y):\n",
    "        self.X_num = X_num\n",
    "        self.X_cat = X_cat\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_num[idx], self.X_cat[idx], self.y[idx]\n",
    "\n",
    "test_dataset = DamEnergyDataset(X_numeric_test, X_categorical_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# =============================\n",
    "# 4. Load trained model\n",
    "# =============================\n",
    "class DamNN(nn.Module):\n",
    "    def __init__(self, n_numeric, categorical_cardinalities, emb_dim=16, hidden_dim=128):\n",
    "        super(DamNN, self).__init__()\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(cardinality, emb_dim) for cardinality in categorical_cardinalities]\n",
    "        )\n",
    "        n_emb = emb_dim * len(categorical_cardinalities)\n",
    "        self.fc1 = nn.Linear(n_numeric + n_emb, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        x = torch.cat([x_num] + embs, dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "categorical_cardinalities = [len(label_encoders[col].classes_) for col in categorical_cols]\n",
    "model = DamNN(len(numeric_cols), categorical_cardinalities)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"../artifacts/best_dam_model.pt\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# =============================\n",
    "# 5. Evaluate\n",
    "# =============================\n",
    "preds, targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_num_batch, X_cat_batch, y_batch in test_loader:\n",
    "        X_num_batch, X_cat_batch = X_num_batch.to(device), X_cat_batch.to(device)\n",
    "        outputs = model(X_num_batch, X_cat_batch)\n",
    "        preds.append(outputs.cpu())\n",
    "        targets.append(y_batch)\n",
    "\n",
    "preds = torch.cat(preds).numpy()\n",
    "targets = torch.cat(targets).numpy()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(targets, preds))\n",
    "mae = mean_absolute_error(targets, preds)\n",
    "r2 = r2_score(targets, preds)\n",
    "\n",
    "print(f\"2022 Test Results | RMSE: {rmse:.2f}, MAE: {mae:.2f}, R²: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c6d00-94e7-46d2-950e-d3e1d310ee7e",
   "metadata": {},
   "source": [
    "# Results from LightGBM\n",
    "Code is taken from <strong><code>Sreelakshmi/lightGBM_test.py</code></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846254db-91b6-4db4-9dd6-e6cf86777e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test rows: 14016\n",
      "loading\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# =============================\n",
    "# 1. Load test data (2022)\n",
    "# =============================\n",
    "test_df = pd.read_csv(\"../data/final_test_2022.csv\")  # Your 2022 data\n",
    "TARGET_COL = \"RectifHyd_MWh\"\n",
    "\n",
    "# Ensure numeric target\n",
    "test_df[TARGET_COL] = pd.to_numeric(test_df[TARGET_COL], errors='coerce')\n",
    "test_df = test_df.dropna(subset=[TARGET_COL])\n",
    "\n",
    "# =============================\n",
    "# 2. Define columns (same as training)\n",
    "# =============================\n",
    "categorical_cols = [\"Division_ID\", \"Primary Purpose\", \"nerc_region\", \"mode\"]\n",
    "numeric_cols = [c for c in test_df.columns if c not in categorical_cols + [TARGET_COL, 'year']]\n",
    "\n",
    "# Ensure numeric features\n",
    "for col in numeric_cols:\n",
    "    test_df[col] = pd.to_numeric(test_df[col], errors='coerce')\n",
    "\n",
    "# =============================\n",
    "# 3. Load preprocessing objects\n",
    "# =============================\n",
    "with open(\"../artifacts/label_encoders.pkl\", \"rb\") as f:  # Updated path\n",
    "    label_encoders = pickle.load(f)\n",
    "scaler = get_scaler()\n",
    "\n",
    "# Fill missing numeric values with median from test set\n",
    "for col in numeric_cols:\n",
    "    test_df[col] = test_df[col].fillna(test_df[col].median())\n",
    "\n",
    "# Encode categorical features using training label encoders\n",
    "for col in categorical_cols:\n",
    "    le = label_encoders[col]\n",
    "    # Handle unseen categories gracefully\n",
    "    test_df[col] = test_df[col].astype(str).map(\n",
    "        lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "    )\n",
    "\n",
    "# Scale numeric features using training scaler\n",
    "test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n",
    "\n",
    "# =============================\n",
    "# 4. Prepare features and target\n",
    "# =============================\n",
    "X_test = test_df[numeric_cols + categorical_cols]\n",
    "y_test = test_df[TARGET_COL].values\n",
    "\n",
    "print(f\"Test rows: {len(test_df)}\")\n",
    "\n",
    "# =============================\n",
    "# 5. Load trained LightGBM model\n",
    "# =============================\n",
    "model_path = \"../artifacts/best_dam_lgbm.model\"\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "\n",
    "gbm = lgb.Booster(model_file=model_path)\n",
    "print(f\"Loaded LightGBM model from {model_path}\")\n",
    "print(f\"Best iteration: {gbm.best_iteration}\")\n",
    "\n",
    "# =============================\n",
    "# 6. Predict & Evaluate\n",
    "# =============================\n",
    "# Predict using best iteration\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "# Compute metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n2022 Test Results (LightGBM)\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE:  {mae:.2f}\")\n",
    "print(f\"R²:   {r2:.3f}\")\n",
    "\n",
    "# =============================\n",
    "# 7. Optional: Feature importance\n",
    "# =============================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lgb.plot_importance(gbm, max_num_features=20, importance_type='gain', figsize=(10, 8))\n",
    "plt.title(\"Top 20 Feature Importance (Gain) - LightGBM\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/final/lightgbm_feature_importance.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887ede24-dd75-4a40-990d-37450179fe7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
